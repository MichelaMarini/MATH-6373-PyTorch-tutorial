{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression vs Non-Linear Regression\n",
        "Linear regression and non-linear regression are both methods used in statistics to model the relationship between a dependent variable and one or more independent variables.\n",
        "\n",
        "\n",
        "*   **Linear regression** assumes that the relationship between the variables is linear, meaning that the change in the dependent variable is directly proportional to the change in the independent variable(s).\n",
        "*   **Non-linear regression** allows for more complex relationships that cannot be adequately described by a straight line. This flexibility enables non-linear regression to capture patterns that linear regression cannot, making it suitable for modeling relationships that are not strictly linear. Common types of non-linear regression models include polynomial regression and exponential regression.\n"
      ],
      "metadata": {
        "id": "dHd3yop4i-Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dataset\n",
        "\n",
        "np.random.seed(0)\n",
        "x_train = 2 * np.random.rand(100, 1).astype(np.float32)  # Generate 100 random numbers between 0 and 2\n",
        "\n",
        "y_train = (4 + 3 * x_train + np.random.randn(100, 1)).astype(np.float32)  # Linear relationship with some noise\n",
        "\n",
        "\n",
        "# Linear regression model\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    inputs = torch.from_numpy(x_train)\n",
        "    targets = torch.from_numpy(y_train)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "# Plot the graph\n",
        "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
        "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
        "plt.plot(x_train, predicted, label='Fitted line')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "metadata": {
        "id": "s7AE4UnX2og2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "# Generate moons dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate moons dataset\n",
        "X_moons, y_moons = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
        "\n",
        "# Plot the train dataset\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], c='blue', marker='o', label='Class 0 (Train)')\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], c='red', marker='s', label='Class 1 (Train)')\n",
        "\n",
        "# Plot the test dataset\n",
        "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c='lightblue', marker='o', label='Class 0 (Test)')\n",
        "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c='salmon', marker='s', label='Class 1 (Test)')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Dataset')\n",
        "plt.show()\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape y_train to [n_samples, 1]\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)    # Reshape y_test to [n_samples, 1]\n",
        "\n",
        "\n",
        "# Define a simple linear regression model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.fc = nn.Linear(2, 1)  # Input size: 2, Output size: 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the linear model, loss function, and optimizer\n",
        "linear_model = LinearRegressionModel()\n",
        "linear_criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "linear_optimizer = optim.Adam(linear_model.parameters(), lr=0.01)  # Adam optimizer\n",
        "\n",
        "# Train the linear model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    linear_optimizer.zero_grad()\n",
        "    linear_outputs = linear_model(X_train_tensor)\n",
        "    linear_loss = linear_criterion(linear_outputs, y_train_tensor)\n",
        "    linear_loss.backward()\n",
        "    linear_optimizer.step()\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Linear Loss: {linear_loss.item():.4f}')\n",
        "\n",
        "\n",
        "# Instantiate the nonlinear model\n",
        "class NonLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NonLinearRegressionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 64)  # Input size: 2, Output size: 64\n",
        "        self.fc2 = nn.Linear(64, 1)   # Input size: 64, Output size: 1\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the nonlinear model, loss function, and optimizer\n",
        "nonlinear_model = NonLinearRegressionModel()\n",
        "nonlinear_criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "nonlinear_optimizer = optim.Adam(nonlinear_model.parameters(), lr=0.01)  # Adam optimizer\n",
        "\n",
        "# Train the nonlinear model\n",
        "for epoch in range(num_epochs):\n",
        "    nonlinear_optimizer.zero_grad()\n",
        "    nonlinear_outputs = nonlinear_model(X_train_tensor)\n",
        "    nonlinear_loss = nonlinear_criterion(nonlinear_outputs, y_train_tensor)\n",
        "    nonlinear_loss.backward()\n",
        "    nonlinear_optimizer.step()\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Nonlinear Loss: {nonlinear_loss.item():.4f}')\n",
        "\n",
        "# Evaluate the models on the test dataset\n",
        "linear_model.eval()\n",
        "nonlinear_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    linear_predicted = linear_model(X_test_tensor).numpy()\n",
        "    nonlinear_predicted = nonlinear_model(X_test_tensor).numpy()\n",
        "\n",
        "# Plot the test dataset and predictions\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=linear_predicted.squeeze(), cmap='coolwarm')\n",
        "plt.title('Linear Regression Predictions')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.colorbar(label='Predicted y')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=nonlinear_predicted.squeeze(), cmap='coolwarm')\n",
        "plt.title('Nonlinear Regression Predictions')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.colorbar(label='Predicted y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8TKeiAL79uZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "A ***feedforward neural network*** is a basic type of artificial neural network where data moves in one direction: forward. It consists of input, hidden, and output layers, processing data through interconnected nodes with weighted sums and activation functions. It's commonly used for tasks like classification and pattern recognition due to its ability to learn complex functions from data."
      ],
      "metadata": {
        "id": "erIDwGbnjJpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='../../data',\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='../../data',\n",
        "    train=False,\n",
        "    transform=transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Data loader\n",
        "\n",
        "subset_indices_train = torch.arange(3000) #3000 images\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=torch.utils.data.SubsetRandomSampler(subset_indices_train)\n",
        ")\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#    dataset=train_dataset,\n",
        "#    batch_size=batch_size,\n",
        "#    shuffle=True\n",
        "#)\n",
        "subset_indices_test = torch.arange(100) # 100 images\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    sampler=torch.utils.data.SubsetRandomSampler(subset_indices_test)\n",
        ")\n",
        "\n",
        "#test_loader = torch.utils.data.DataLoader(\n",
        "#    dataset=test_dataset,\n",
        "#    batch_size=batch_size,\n",
        "#    shuffle=False\n",
        "#)\n",
        "\n",
        "# Fully connected neural network with varying hidden layers\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "def train_model(num_epochs, hidden_size):\n",
        "    model = FeedForwardNN(input_size, hidden_size, num_classes).to(device)  # Initialize the model\n",
        "    criterion = nn.CrossEntropyLoss()  # Define loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Define optimizer\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.reshape(-1, 28 * 28).to(device)  # Reshape images\n",
        "            labels = labels.to(device)  # Move labels to the device\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return model, train_losses\n",
        "\n",
        "\n",
        "def evaluate_model(model):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
        "        for images, labels in test_loader:\n",
        "            images = images.reshape(-1, 28 * 28).to(device)  # Reshape images\n",
        "            labels = labels.to(device)  # Move labels to the device\n",
        "            outputs = model(images)  # Forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    precision = precision_score(true_labels, predictions, average='weighted')\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "\n",
        "    return accuracy, precision\n",
        "\n",
        "hidden_layers = [8, 20, 40, 100]  # Different hidden layer sizes\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # Create a 2x2 grid of subplots\n",
        "\n",
        "for i, hidden_size in enumerate(hidden_layers):\n",
        "    print(\"number of neurons:\", hidden_size)\n",
        "    ax = axs[i // 2, i % 2]  # Select the current subplot\n",
        "    trained_model, train_losses = train_model(num_epochs, hidden_size)\n",
        "    accuracy, precision = evaluate_model(trained_model)\n",
        "    ax.plot(range(1, num_epochs + 1), train_losses, marker='o', label=f'Hidden Size: {hidden_size}, Accuracy: {accuracy:.4f}')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Training Loss')\n",
        "    ax.set_title(f'Training Loss vs. Number of Neurons (Hidden Size: {hidden_size})')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X-q6Sn3vCTlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_model(num_epochs):\n",
        "    model = FeedForwardNN(input_size, hidden_size, num_classes).to(device)  # Initialize the model\n",
        "    criterion = nn.CrossEntropyLoss()  # Define loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Define optimizer\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)  # Reshape images\n",
        "            labels = labels.to(device)  # Move labels to the device\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # plt.plot(range(1, num_epochs + 1), train_losses, label=f'{num_epochs} Epochs')\n",
        "\n",
        "    return model, train_losses\n",
        "\n",
        "\n",
        "def evaluate_model(model):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
        "        for images, labels in test_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)  # Reshape images\n",
        "            labels = labels.to(device)  # Move labels to the device\n",
        "            outputs = model(images)  # Forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    precision = precision_score(true_labels, predictions, average='weighted')\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "\n",
        "    return accuracy, precision\n",
        "\n",
        "accuracies = []\n",
        "precisions = []\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # Create a 2x2 grid of subplots\n",
        "\n",
        "for i, num_epochs in enumerate([1, 2, 3, 4]): #[5, 10, 15, 20]\n",
        "    ax = axs[i // 2, i % 2]  # Select the current subplot\n",
        "    trained_model, train_losses = train_model(num_epochs)\n",
        "    accuracy, precision = evaluate_model(trained_model)\n",
        "    ax.plot(range(1, num_epochs + 1), train_losses, marker='o', label=f'{num_epochs} Epochs')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Training Loss')\n",
        "    ax.set_title(f'Training Loss vs. Number of Epochs ({num_epochs} Epochs)')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UImtRb2Bn3pR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}